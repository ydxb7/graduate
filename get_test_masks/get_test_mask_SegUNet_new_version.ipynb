{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# plt.rcParams['figure.figsize'] = (20, 20)\n",
    "# plt.rcParams['image.interpolation'] = 'bilinear'\n",
    "\n",
    "import sys\n",
    "sys.path.append('../train/')\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import collections\n",
    "import numbers\n",
    "import random\n",
    "import math\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from networks.SegUNet_new_version import SegUNet_new_version\n",
    "import tool\n",
    "from tqdm import tqdm\n",
    "\n",
    "flip_index = ['16', '15', '14', '13', '12', '11', '10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 3\n",
    "NUM_CLASSES = 2 \n",
    "BATCH_SIZE = 8\n",
    "W, H = 1918, 1280\n",
    "STRIDE = 256\n",
    "IMAGE_SIZE = 512\n",
    "test_mask_path = '../../data/test_masks/SegUNet_new_version/'\n",
    "weight_path = '../_weights/SegUNet_new_version-fold0-0.00497.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename, model):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SegUNet_new_version(num_classes=NUM_CLASSES)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "load_model(weight_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '../../data/images/test/'\n",
    "\n",
    "if not os.path.exists(test_mask_path):\n",
    "    os.makedirs(test_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names = os.listdir(test_path)\n",
    "test_names = sorted(test_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch_size = BATCH_SIZE\n",
    "    normalize_mean = [.485, .456, .406]\n",
    "    normalize_std = [.229, .224, .225]\n",
    "\n",
    "    test_names = sorted(os.listdir(test_path))\n",
    "    for image_pack in tqdm(range(len(test_names) // batch_size)):\n",
    "        images = np.zeros((batch_size, 3, H, W), dtype='float32')\n",
    "        test_masks = np.zeros((batch_size, 2, H, W), dtype='float32')\n",
    "        ifflip = [False] * batch_size\n",
    "        image_batch_names = test_names[image_pack * batch_size: image_pack * batch_size + batch_size]\n",
    "        mask_names = [input_name.split('.')[0] + '.png' for input_name in image_batch_names]\n",
    "        \n",
    "        for idx, image_name in enumerate(image_batch_names):\n",
    "            image = Image.open(os.path.join(test_path, image_name))\n",
    "            angle = image_name.split('.')[0].split('_')[-1]\n",
    "            if angle in flip_index:\n",
    "                ifflip[idx] = True\n",
    "                image = ImageOps.mirror(image)\n",
    "\n",
    "            image = np.array(image).astype('float') / 255\n",
    "            image = image.transpose(2, 0, 1)\n",
    "\n",
    "            for i in range(3):\n",
    "                image[i] = (image[i] - normalize_mean[i]) / normalize_std[i]\n",
    "\n",
    "            images[idx] = image\n",
    "\n",
    "        for h_idx in range(int(math.ceil((H - STRIDE) / STRIDE))):\n",
    "            h_start = h_idx * STRIDE\n",
    "            h_end = h_start + IMAGE_SIZE\n",
    "            if h_end > H:\n",
    "                h_end = H\n",
    "                h_start = h_end - IMAGE_SIZE\n",
    "            for w_idx in range(int(math.ceil((W - STRIDE) / STRIDE))):\n",
    "                w_start = w_idx * STRIDE\n",
    "                w_end = w_start + IMAGE_SIZE\n",
    "                if w_end > W:\n",
    "                    w_end = W\n",
    "                    w_start = w_end - IMAGE_SIZE\n",
    "\n",
    "                input_batchs = images[:, :, h_start:h_end, w_start:w_end]\n",
    "                input_tensor = torch.from_numpy(input_batchs).cuda()\n",
    "\n",
    "                inputs = Variable(input_tensor, )\n",
    "                outputs = model(inputs)\n",
    "                ouputs = outputs.cpu().data.numpy()\n",
    "\n",
    "                test_masks[:, :, h_start:h_end, w_start:w_end] += ouputs\n",
    "        \n",
    "        test_masks = np.argmax(test_masks, axis=1).astype('uint8')\n",
    "        for idx in range(batch_size):\n",
    "            output_PIL = Image.fromarray(test_masks[idx].astype('uint8')*255).convert('1')\n",
    "            if ifflip[idx]:\n",
    "                output_PIL = ImageOps.mirror(output_PIL)\n",
    "            mask_name = mask_names[idx]\n",
    "            output_PIL.save(test_mask_path + mask_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
